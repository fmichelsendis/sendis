---
output: rmarkdown::html_document 
---

[![Build Status](https://travis-ci.org/fmichelsendis/sendis.svg?branch=master)](https://travis-ci.org/fmichelsendis/sendis)
[![Github All Releases](https://img.shields.io/github/downloads/fmichelsendis/sendis/total.svg)]()
 
<br>
<img src="man/figures/sendis.png" align="left" height = 35/> 

<br><br>  

<p style="color:#70797f;font-weight:100; font-size: large;
font-family:Helvetica"> Screening evaluated nuclear data integral performance </p>




```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  echo=FALSE, 
  message=FALSE, 
  warning=FALSE, 
  eval=TRUE,
  fig.align='left'
  ) 

library(sendis)
library(dplyr)
library(plotly)
library(widgetframe)

```

<img src="examples/plot1.png" align="center" width="25%">
<img src="examples/plot2.png" align="center" width="25%"> 
<img src="examples/plot3.png" align="center" width="25%">
<img src="examples/plot4.png" align="center" width="25%"> 

<br>

The **sendis** R package provides an easy access to datasets and functions that expedite common tasks in the comparison of integral benchmarking performance of nuclear data libraries.

## Background

The ability of a nuclear data library to accurately reproduce, in particle-transport simulations, the observables of benchmark-quality integral experiments plays a decisive role in the data validation process and therefore also in the decision-making process of assembling any nuclear data library. 

The work presented here aims at providing elements of answers to the following questions: 

* How do different nuclear data libraries compare in terms of integral performance? 
* How has the integral performance of evaluated nuclear data libraries evolved across different releases?
* How do two assessments provided by two different benchmarking *suites* compare to each other?
* How to build a consistend benchmarking comparison across different assessments done by different institutions?
* How to spot general trends and outliers? 

For the comparison of critical experiments, some helpful tools have been developed in this work.  

### Sources 


* [DICE](https://www.oecd-nea.org/science/wpncs/icsbep/dice.html), the OECD NEA database of the [International Criticality Safety Benchmark Evaluation Project](http://www.oecd-nea.org/science/wpncs/icsbep).
 
## Examples

Interactive graphs are 

```{r,make myplotly.html, fig.align='center', eval=FALSE}

data(sendis) 
df<-filter(sendis, INST=="NEA", VER!="2.2")
p1<-plot_cumulchi(df)%>%
  layout(xaxis = list(title = "Benchmark suite"),
         yaxis = list(title = TeX("\\chi^2")))%>%
  config(mathjax = "cdn")
# to display later using iframe, in order to render Mathjax : 
htmlwidgets::saveWidget(p1, "myplot.html")

```
 
<iframe src="myplot.html" width="700" height="450" scrolling="no" seamless="seamless" frameBorder="0"> </iframe>
 
## Reproducible, automated reporting 

[Rmarkdown](https://rmarkdown.rstudio.com/)  


## The sendis app 

For seeing trends in the data and filtering through the data, an interactive Shiny application has been developed. Find out more about the [sendis app](https://sendislab.org/app.html).  
 
 








